# 高性能法律智能问答系统解决方案

针对您提到的两个核心问题，我设计了一个完整的解决方案。这个系统结合了向量数据库、语义检索和混合搜索技术，能够高效处理大量数据并提供精准的法律建议。

## 系统架构设计

```python
import numpy as np
import pandas as pd
import json
import os
import time
from typing import List, Dict, Any, Tuple
import pymysql
from sentence_transformers import SentenceTransformer
import faiss
from datetime import datetime
import jieba
import jieba.analyse
from openai import OpenAI

class HighPerformanceLegalQA:
    def __init__(self, mysql_config: Dict, laws_folder: str):
        """
        初始化高性能法律问答系统
        
        Args:
            mysql_config: MySQL数据库连接配置
            laws_folder: 法律法规文件夹路径
        """
        self.mysql_config = mysql_config
        self.laws_folder = laws_folder
        
        # 初始化模型
        print("加载语义模型...")
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # 初始化向量数据库
        self.vector_dim = 384  # 模型输出维度
        self.case_index = None
        self.law_index = None
        self.case_data = []
        self.law_data = []
        
        # 初始化MySQL连接
        self.mysql_conn = None
        
        # 初始化DeepSeek客户端
        self.client = OpenAI(
            api_key="sk-63b9590c84624562bb4dcf915d5ccf65",
            base_url="https://api.deepseek.com"
        )
        
        # 加载数据
        self.load_data()
    
    def connect_mysql(self):
        """连接MySQL数据库"""
        try:
            self.mysql_conn = pymysql.connect(**self.mysql_config)
            return True
        except Exception as e:
            print(f"MySQL连接失败: {e}")
            return False
    
    def load_case_data(self):
        """从MySQL加载案例数据并构建向量索引"""
        if not self.connect_mysql():
            return False
        
        try:
            print("从MySQL加载案例数据...")
            cursor = self.mysql_conn.cursor()
            
            # 分批读取数据，避免内存溢出
            batch_size = 10000
            offset = 0
            all_texts = []
            all_metadata = []
            
            while True:
                query = f"""
                SELECT id, title, description, solution, category, create_time, update_time 
                FROM legal_cases 
                LIMIT {batch_size} OFFSET {offset}
                """
                cursor.execute(query)
                batch_results = cursor.fetchall()
                
                if not batch_results:
                    break
                
                for row in batch_results:
                    case_id, title, description, solution, category, create_time, update_time = row
                    
                    # 组合文本用于语义搜索
                    combined_text = f"{title} {description} {solution}"
                    all_texts.append(combined_text)
                    
                    # 保存元数据
                    metadata = {
                        'id': case_id,
                        'title': title,
                        'description': description,
                        'solution': solution,
                        'category': category,
                        'create_time': create_time,
                        'update_time': update_time,
                        'type': 'case'
                    }
                    all_metadata.append(metadata)
                
                offset += batch_size
                print(f"已加载 {offset} 条案例数据...")
            
            cursor.close()
            
            if not all_texts:
                print("未找到案例数据")
                return False
            
            # 构建向量索引
            print("构建案例向量索引...")
            embeddings = self.embedding_model.encode(all_texts, show_progress_bar=True)
            
            # 创建FAISS索引
            self.case_index = faiss.IndexFlatIP(self.vector_dim)
            self.case_index.add(embeddings.astype('float32'))
            
            self.case_data = all_metadata
            print(f"案例索引构建完成，共 {len(self.case_data)} 条数据")
            return True
            
        except Exception as e:
            print(f"加载案例数据失败: {e}")
            return False
    
    def load_law_data(self):
        """加载法律法规数据并构建向量索引"""
        print("加载法律法规数据...")
        all_texts = []
        all_metadata = []
        
        if not os.path.exists(self.laws_folder):
            print(f"法律法规文件夹不存在: {self.laws_folder}")
            return False
        
        for filename in os.listdir(self.laws_folder):
            if filename.endswith('.txt'):
                file_path = os.path.join(self.laws_folder, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        law_data = json.loads(content)
                        
                        # 提取关键信息
                        law_id = law_data.get('id', filename)
                        title = law_data.get('title', '')
                        content_text = law_data.get('content', '')
                        issue_date = law_data.get('issue_date', '')
                        effective_date = law_data.get('effective_date', '')
                        category = law_data.get('category', '')
                        
                        # 组合文本用于语义搜索
                        combined_text = f"{title} {content_text}"
                        all_texts.append(combined_text)
                        
                        # 保存元数据
                        metadata = {
                            'id': law_id,
                            'title': title,
                            'content': content_text,
                            'issue_date': issue_date,
                            'effective_date': effective_date,
                            'category': category,
                            'type': 'law',
                            'filename': filename
                        }
                        all_metadata.append(metadata)
                        
                except Exception as e:
                    print(f"解析法律法规文件失败 {filename}: {e}")
        
        if not all_texts:
            print("未找到法律法规数据")
            return False
        
        # 构建向量索引
        print("构建法律法规向量索引...")
        embeddings = self.embedding_model.encode(all_texts, show_progress_bar=True)
        
        # 创建FAISS索引
        self.law_index = faiss.IndexFlatIP(self.vector_dim)
        self.law_index.add(embeddings.astype('float32'))
        
        self.law_data = all_metadata
        print(f"法律法规索引构建完成，共 {len(self.law_data)} 条数据")
        return True
    
    def load_data(self):
        """加载所有数据"""
        self.load_case_data()
        self.load_law_data()
    
    def semantic_search(self, query: str, top_k: int = 5, data_type: str = 'both') -> List[Dict]:
        """
        语义搜索
        
        Args:
            query: 查询文本
            top_k: 返回结果数量
            data_type: 搜索类型 ('case', 'law', 'both')
        """
        start_time = time.time()
        
        # 生成查询向量
        query_embedding = self.embedding_model.encode([query]).astype('float32')
        
        results = []
        
        # 搜索案例数据
        if data_type in ['case', 'both'] and self.case_index is not None:
            case_scores, case_indices = self.case_index.search(query_embedding, top_k)
            for score, idx in zip(case_scores[0], case_indices[0]):
                if idx < len(self.case_data):
                    result = self.case_data[idx].copy()
                    result['score'] = float(score)
                    result['search_type'] = 'semantic'
                    results.append(result)
        
        # 搜索法律法规数据
        if data_type in ['law', 'both'] and self.law_index is not None:
            law_scores, law_indices = self.law_index.search(query_embedding, top_k)
            for score, idx in zip(law_scores[0], law_indices[0]):
                if idx < len(self.law_data):
                    result = self.law_data[idx].copy()
                    result['score'] = float(score)
                    result['search_type'] = 'semantic'
                    results.append(result)
        
        # 按相关性得分排序
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # 取前top_k个结果
        final_results = results[:top_k]
        
        search_time = time.time() - start_time
        print(f"语义搜索完成，耗时: {search_time:.2f}秒")
        
        return final_results
    
    def keyword_search(self, query: str, top_k: int = 5, data_type: str = 'both') -> List[Dict]:
        """
        关键词搜索
        
        Args:
            query: 查询文本
            top_k: 返回结果数量
            data_type: 搜索类型 ('case', 'law', 'both')
        """
        start_time = time.time()
        
        # 使用jieba提取关键词
        keywords = jieba.analyse.extract_tags(query, topK=10)
        
        results = []
        
        # 搜索案例数据
        if data_type in ['case', 'both']:
            for case in self.case_data:
                score = 0
                text = f"{case['title']} {case['description']} {case['solution']} {case['category']}".lower()
                
                for keyword in keywords:
                    if keyword.lower() in text:
                        score += 1
                
                if score > 0:
                    result = case.copy()
                    result['score'] = score
                    result['search_type'] = 'keyword'
                    results.append(result)
        
        # 搜索法律法规数据
        if data_type in ['law', 'both']:
            for law in self.law_data:
                score = 0
                text = f"{law['title']} {law['content']} {law['category']}".lower()
                
                for keyword in keywords:
                    if keyword.lower() in text:
                        score += 1
                
                if score > 0:
                    result = law.copy()
                    result['score'] = score
                    result['search_type'] = 'keyword'
                    results.append(result)
        
        # 按得分排序
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # 取前top_k个结果
        final_results = results[:top_k]
        
        search_time = time.time() - start_time
        print(f"关键词搜索完成，耗时: {search_time:.2f}秒")
        
        return final_results
    
    def hybrid_search(self, query: str, top_k: int = 5, semantic_weight: float = 0.7) -> List[Dict]:
        """
        混合搜索 - 结合语义搜索和关键词搜索
        
        Args:
            query: 查询文本
            top_k: 返回结果数量
            semantic_weight: 语义搜索权重 (0-1)
        """
        # 并行执行两种搜索
        semantic_results = self.semantic_search(query, top_k * 2, 'both')
        keyword_results = self.keyword_search(query, top_k * 2, 'both')
        
        # 合并结果并去重
        all_results = {}
        
        # 处理语义搜索结果
        for result in semantic_results:
            result_id = f"{result['type']}_{result['id']}"
            if result_id not in all_results:
                all_results[result_id] = result
                all_results[result_id]['final_score'] = result['score'] * semantic_weight
            else:
                all_results[result_id]['final_score'] += result['score'] * semantic_weight
        
        # 处理关键词搜索结果
        keyword_weight = 1 - semantic_weight
        for result in keyword_results:
            result_id = f"{result['type']}_{result['id']}"
            if result_id in all_results:
                all_results[result_id]['final_score'] += result['score'] * keyword_weight
            else:
                all_results[result_id] = result
                all_results[result_id]['final_score'] = result['score'] * keyword_weight
        
        # 转换为列表并按最终得分排序
        final_results = list(all_results.values())
        final_results.sort(key=lambda x: x['final_score'], reverse=True)
        
        return final_results[:top_k]
    
    def get_latest_laws(self, category: str = None, limit: int = 5) -> List[Dict]:
        """
        获取最新的法律法规
        
        Args:
            category: 法规类别
            limit: 返回数量
        """
        laws = self.law_data.copy()
        
        # 按发布日期排序
        laws.sort(key=lambda x: x.get('issue_date', ''), reverse=True)
        
        # 筛选类别
        if category:
            laws = [law for law in laws if law.get('category') == category]
        
        return laws[:limit]
    
    def find_most_relevant_solutions(self, query: str, top_k: int = 3) -> Dict:
        """
        找到最相关的解决方案
        
        Args:
            query: 查询文本
            top_k: 返回结果数量
        """
        # 使用混合搜索找到最相关的内容
        search_results = self.hybrid_search(query, top_k * 2)
        
        # 分类结果
        cases = [r for r in search_results if r['type'] == 'case']
        laws = [r for r in search_results if r['type'] == 'law']
        
        # 获取最新的法律法规
        latest_laws = self.get_latest_laws(limit=top_k)
        
        return {
            'relevant_cases': cases[:top_k],
            'relevant_laws': laws[:top_k],
            'latest_laws': latest_laws
        }
    
    def generate_legal_advice(self, query: str) -> str:
        """
        生成法律建议
        
        Args:
            query: 用户问题
        """
        # 找到最相关的内容
        relevant_content = self.find_most_relevant_solutions(query)
        
        # 构建上下文
        context = "以下是从法律知识库中检索到的相关信息：\n\n"
        
        # 添加相关案例
        if relevant_content['relevant_cases']:
            context += "相关案例及解决方案：\n"
            for i, case in enumerate(relevant_content['relevant_cases'], 1):
                context += f"{i}. {case['title']}\n"
                context += f"   描述: {case['description'][:200]}...\n"
                context += f"   解决方案: {case['solution'][:200]}...\n\n"
        
        # 添加相关法律法规
        if relevant_content['relevant_laws']:
            context += "相关法律法规：\n"
            for i, law in enumerate(relevant_content['relevant_laws'], 1):
                context += f"{i}. {law['title']}\n"
                context += f"   内容: {law['content'][:300]}...\n\n"
        
        # 添加最新法律法规
        if relevant_content['latest_laws']:
            context += "最新法律法规：\n"
            for i, law in enumerate(relevant_content['latest_laws'], 1):
                context += f"{i}. {law['title']} (发布日期: {law.get('issue_date', '未知')})\n"
                context += f"   内容: {law['content'][:200]}...\n\n"
        
        # 调用大模型生成回答
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "你是一个专业的法律顾问，请根据提供的法律案例和法规，给出专业、准确的法律建议。"},
                    {"role": "user", "content": f"问题：{query}\n\n{context}\n请基于以上信息，给出专业的法律建议："}
                ],
                stream=False
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"生成建议时出现错误: {str(e)}"
    
    def search_with_filters(self, query: str, category: str = None, 
                          min_date: str = None, max_date: str = None, 
                          search_type: str = 'hybrid') -> List[Dict]:
        """
        带过滤条件的搜索
        
        Args:
            query: 查询文本
            category: 类别过滤
            min_date: 最小日期
            max_date: 最大日期
            search_type: 搜索类型 ('semantic', 'keyword', 'hybrid')
        """
        # 执行搜索
        if search_type == 'semantic':
            results = self.semantic_search(query, 20, 'both')
        elif search_type == 'keyword':
            results = self.keyword_search(query, 20, 'both')
        else:
            results = self.hybrid_search(query, 20)
        
        # 应用过滤器
        filtered_results = []
        
        for result in results:
            # 类别过滤
            if category and result.get('category') != category:
                continue
            
            # 日期过滤
            date_field = 'update_time' if result['type'] == 'case' else 'issue_date'
            item_date = result.get(date_field, '')
            
            if min_date and item_date < min_date:
                continue
                
            if max_date and item_date > max_date:
                continue
            
            filtered_results.append(result)
        
        return filtered_results[:10]  # 返回前10个结果


# 使用示例
def main():
    # MySQL配置
    mysql_config = {
        'host': 'localhost',
        'user': 'your_username',
        'password': 'your_password',
        'database': 'legal_cases',
        'charset': 'utf8mb4'
    }
    
    # 法律法规文件夹路径
    laws_folder = './laws/'
    
    # 初始化系统
    print("初始化法律问答系统...")
    legal_qa = HighPerformanceLegalQA(mysql_config, laws_folder)
    
    # 测试查询
    while True:
        query = input("\n请输入法律问题 (输入'退出'结束): ").strip()
        
        if query.lower() in ['退出', 'quit', 'exit']:
            break
        
        if not query:
            continue
        
        print("\n正在搜索相关信息...")
        start_time = time.time()
        
        # 生成法律建议
        advice = legal_qa.generate_legal_advice(query)
        
        end_time = time.time()
        print(f"\n生成建议耗时: {end_time - start_time:.2f}秒")
        print(f"\n法律建议:\n{advice}")
        
        # 显示搜索结果的详细信息
        relevant_content = legal_qa.find_most_relevant_solutions(query)
        print(f"\n找到 {len(relevant_content['relevant_cases'])} 个相关案例")
        print(f"找到 {len(relevant_content['relevant_laws'])} 个相关法规")
        print(f"找到 {len(relevant_content['latest_laws'])} 个最新法规")

if __name__ == "__main__":
    main()
```

## 性能优化方案

### 1. 向量索引优化

```python
class OptimizedLegalQA(HighPerformanceLegalQA):
    def __init__(self, mysql_config: Dict, laws_folder: str):
        super().__init__(mysql_config, laws_folder)
        
    def create_optimized_index(self):
        """创建优化的向量索引"""
        # 使用HNSW索引提高搜索速度
        if self.case_index is not None:
            # 重建案例索引为HNSW
            self.case_index = faiss.IndexHNSWFlat(self.vector_dim, 32)
            
            # 重新添加向量
            texts = [f"{case['title']} {case['description']} {case['solution']}" 
                    for case in self.case_data]
            embeddings = self.embedding_model.encode(texts)
            self.case_index.add(embeddings.astype('float32'))
        
        if self.law_index is not None:
            # 重建法规索引为HNSW
            self.law_index = faiss.IndexHNSWFlat(self.vector_dim, 32)
            
            # 重新添加向量
            texts = [f"{law['title']} {law['content']}" for law in self.law_data]
            embeddings = self.embedding_model.encode(texts)
            self.law_index.add(embeddings.astype('float32'))
    
    def batch_semantic_search(self, queries: List[str], top_k: int = 5) -> List[List[Dict]]:
        """批量语义搜索 - 提高处理效率"""
        query_embeddings = self.embedding_model.encode(queries).astype('float32')
        
        all_results = []
        
        # 批量搜索案例
        if self.case_index is not None:
            case_scores, case_indices = self.case_index.search(query_embeddings, top_k)
            for i, query in enumerate(queries):
                query_results = []
                for score, idx in zip(case_scores[i], case_indices[i]):
                    if idx < len(self.case_data):
                        result = self.case_data[idx].copy()
                        result['score'] = float(score)
                        query_results.append(result)
                all_results.append(query_results)
        
        return all_results
```

### 2. 缓存机制

```python
import hashlib
from functools import lru_cache

class CachedLegalQA(HighPerformanceLegalQA):
    def __init__(self, mysql_config: Dict, laws_folder: str):
        super().__init__(mysql_config, laws_folder)
        self.query_cache = {}
    
    def get_query_hash(self, query: str, search_type: str) -> str:
        """生成查询哈希"""
        return hashlib.md5(f"{query}_{search_type}".encode()).hexdigest()
    
    @lru_cache(maxsize=1000)
    def cached_semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """带缓存的语义搜索"""
        return self.semantic_search(query, top_k)
    
    @lru_cache(maxsize=1000)
    def cached_hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """带缓存的混合搜索"""
        return self.hybrid_search(query, top_k)
```

## 部署和使用建议

### 1. 系统要求
- Python 3.8+
- 至少16GB内存（用于处理20万+数据）
- SSD硬盘（提高向量索引加载速度）

### 2. 安装依赖
```bash
pip install sentence-transformers faiss-cpu pymysql jieba pandas numpy openai
```

### 3. 数据预处理脚本

```python
# data_preprocessor.py
import pymysql
import json

def preprocess_mysql_data(mysql_config, output_file='case_data.json'):
    """预处理MySQL数据"""
    conn = pymysql.connect(**mysql_config)
    cursor = conn.cursor()
    
    # 分批处理数据
    batch_size = 5000
    offset = 0
    all_cases = []
    
    while True:
        query = f"SELECT * FROM legal_cases LIMIT {batch_size} OFFSET {offset}"
        cursor.execute(query)
        results = cursor.fetchall()
        
        if not results:
            break
        
        for row in results:
            case_dict = {
                'id': row[0],
                'title': row[1],
                'description': row[2],
                'solution': row[3],
                'category': row[4],
                'create_time': str(row[5]),
                'update_time': str(row[6])
            }
            all_cases.append(case_dict)
        
        offset += batch_size
        print(f"已处理 {offset} 条数据")
    
    # 保存预处理数据
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_cases, f, ensure_ascii=False, indent=2)
    
    cursor.close()
    conn.close()
    print(f"数据预处理完成，共 {len(all_cases)} 条案例")
```

这个解决方案能够：
1. **大幅提高检索速度**：通过向量索引将检索时间从分钟级降低到秒级
2. **精准匹配最新法规**：结合语义搜索和时效性过滤
3. **智能混合搜索**：同时考虑语义相关性和关键词匹配
4. **提供专业建议**：基于检索结果生成准确的法律建议

系统在处理20万+数据时，典型查询响应时间应该在1-3秒内，相比直接查询数据库有数量级的性能提升。